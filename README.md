# TVS Loan Default Prediction

## ğŸ“Œ Introduction
This repository provides an end-to-end machine learning pipeline for predicting loan defaults using **TVS Creditâ€™s motorcycle loan dataset**. It benchmarks **gradient-boosted trees** (XGBoost) against several deep learning models (MLP, 1D CNN, LSTM), and evaluates them with both traditional metrics (ROC-AUC, PR-AUC) and a **custom business-weighted scoring function** that penalizes false positives more heavily â€” mimicking real-world lending risks.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ TVS_Loan_Default_Prediction.ipynb   # Jupyter notebook with full workflow
â”œâ”€â”€ data/                               # Dataset directory (contains TVS.csv)
â”œâ”€â”€ best_model_xgb.joblib               # Persisted XGBoost model (if selected)
â”œâ”€â”€ best_nn_model.keras                 # Persisted neural network model (if selected)
â”œâ”€â”€ requirements.txt                    # Project dependencies
â””â”€â”€ README.md                           # This documentation
```

---

## âš™ï¸ Setup

### Clone the repository
```bash
git clone <repo-url>
cd tvs-loan-default-prediction
```

### Install dependencies
```bash
pip install -r requirements.txt
```

### Prepare data
- Download `TVS.csv` into the `data/` folder.
- Ensure it contains exactly **32 columns** including the `Loan_Default` target.

---

## ğŸ§  Notebook Workflow

### ğŸ”§ Installation
Installs:
- `xgboost`, `optuna`, `imbalanced-learn`
- `scikit-learn`, `tensorflow`, `joblib`, `matplotlib`, etc.

### ğŸ“Š Data Loading & EDA
- Loads `data/TVS.csv`
- Prints dataset shape & summary stats
- Visualizes class imbalance

### ğŸ—ï¸ Preprocessing Pipeline
- Numeric: `StandardScaler`
- Categorical: `OneHotEncoder`
- Combined using `ColumnTransformer`

### ğŸ¯ Baseline Model: XGBoost
- Trained with `SMOTE` for handling imbalance
- Evaluated with ROC-AUC & PR-AUC

### ğŸ” Hyperparameter Tuning
- `Optuna` runs 25 trials
- Optimizes based on **cross-validated ROC-AUC**

### ğŸ¤– Neural Network Architectures
- **MLP**: Dense layers with BatchNorm + Dropout
- **1D CNN**: Treats features as 1D sequence
- **LSTM**: Reshapes flat input into sequences

All trained with `EarlyStopping` and evaluated on ROC-AUC.

### ğŸ’¼ Business-Weighted Cost Metric
Custom function:
```
score = TP âˆ’ 5 Ã— FP
```
Reflects that **false positives (approving a defaulter)** are costlier than **false negatives**.

### ğŸ’¾ Model Persistence
- Saves best model:
  - `.joblib` for XGBoost
  - `.keras` for neural networks

---

## ğŸ“ˆ Evaluation Metrics

| Metric         | Description                                      |
|----------------|--------------------------------------------------|
| **ROC-AUC**    | Area under ROC Curve (separates classes well)    |
| **PR-AUC**     | Area under Precision-Recall Curve (for imbalance)|
| **Business Score** | Custom metric: `TP âˆ’ 5 Ã— FP`                   |

---

## ğŸš€ Usage

### Run the notebook
```bash
jupyter notebook TVS_Loan_Default_Prediction.ipynb
```

### Retrain with new data
1. Replace or add your new `.csv` inside `data/`
2. Update path inside the notebook
3. Run all cells

### Load persisted model for inference

```python
import joblib, pandas as pd
model = joblib.load('best_model_xgb.joblib')
df_new = pd.read_csv('data/new_batch.csv')
preds = model.predict_proba(df_new)[:, 1]
```

---

## âœ… Results Summary

| Model     | ROC-AUC | PR-AUC | Business Score |
|-----------|---------|--------|----------------|
| XGBoost   | 0.8016  | 0.1827 | -34            |
| MLP       | 0.6236  | 0.1332 | 0              |
| 1-D CNN   | 0.6520  | 0.1384 | 0              |
| LSTM      | 0.4782  | 0.0931 | 0              |

> âš ï¸ Note: Negative business score means false positives outweigh true positives.

---

## ğŸ”® Future Directions

- Explore **transformer-based tabular models** (e.g. FTTransformer, SAINT)
- Try **model ensembling** (blend XGBoost + MLP for better recall)
- Improve **LSTM performance** using actual temporal features
- Add **streamlit dashboard** for internal business users
- Deploy via **FastAPI** or **Flask** for API-based predictions

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.
