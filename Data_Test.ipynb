{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37fa367c",
   "metadata": {},
   "source": [
    "# TVS Loan Default Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945caf3",
   "metadata": {},
   "source": [
    "## Install Required Libraries\n",
    "\n",
    "Before starting, install all necessary Python libraries for data analysis and machine learning. This ensures that the environment has the latest versions of the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "409a1e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (1.26.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (3.10.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\psp17\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy seaborn matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b175a7",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Let's import all the necessary libraries for data manipulation, visualization, and feature selection. This step ensures that all required modules are available for the subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d31263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Show full columns\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b253a8a6",
   "metadata": {},
   "source": [
    "Now that we've imported the required libraries, we're ready to load and explore the dataset in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b834d2f",
   "metadata": {},
   "source": [
    "## Load and Inspect the Dataset\n",
    "\n",
    "In this step, we'll load the dataset from a CSV file, assign meaningful column names, and perform an initial inspection. This helps us understand the structure of the data and identify any missing values or data type issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c368a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Dataset: (119528, 32)\n",
      "\n",
      "Missing Values:\n",
      " Customer_ID                                      0\n",
      "First_EMI_Bounced                                0\n",
      "Bounces_Last_12_Months                           0\n",
      "Max_MOB                                      34480\n",
      "Total_Bounces_On_Loan                        34480\n",
      "EMI                                          34480\n",
      "Loan_Amount                                  34480\n",
      "Tenure                                       34480\n",
      "Dealer_Code                                  34480\n",
      "Product_Code                                 34480\n",
      "Advance_EMIs_Paid                            34480\n",
      "Interest_Rate                                34480\n",
      "Gender                                       34480\n",
      "Employment_Type                              34480\n",
      "Resident_Type                                35397\n",
      "DOB                                          34480\n",
      "Loan_Age_At_Application                      34480\n",
      "Total_Loans                                      0\n",
      "Secured_Loans                                    0\n",
      "Unsecured_Loans                                  0\n",
      "Max_Sanctioned_Amount_Live                   82902\n",
      "New_Loans_Last_3_Months                          0\n",
      "Total_Secured_Sanctioned_Live               100247\n",
      "Total_Unsecured_Sanctioned_Live             100500\n",
      "Max_TwoWheeler_Loan_Amount                   15061\n",
      "Months_Since_Last_Personal_Loan             106097\n",
      "Months_Since_First_Consumer_Durable_Loan     99095\n",
      "DPD_30_Last_6_Months                             0\n",
      "DPD_60_Last_6_Months                             0\n",
      "DPD_90_Last_3_Months                             0\n",
      "Tier                                             0\n",
      "Loan_Default                                     0\n",
      "dtype: int64\n",
      "\n",
      "Data Types:\n",
      " Customer_ID                                   int64\n",
      "First_EMI_Bounced                             int64\n",
      "Bounces_Last_12_Months                        int64\n",
      "Max_MOB                                     float64\n",
      "Total_Bounces_On_Loan                       float64\n",
      "EMI                                         float64\n",
      "Loan_Amount                                 float64\n",
      "Tenure                                      float64\n",
      "Dealer_Code                                 float64\n",
      "Product_Code                                 object\n",
      "Advance_EMIs_Paid                           float64\n",
      "Interest_Rate                               float64\n",
      "Gender                                       object\n",
      "Employment_Type                              object\n",
      "Resident_Type                                object\n",
      "DOB                                          object\n",
      "Loan_Age_At_Application                     float64\n",
      "Total_Loans                                   int64\n",
      "Secured_Loans                                 int64\n",
      "Unsecured_Loans                               int64\n",
      "Max_Sanctioned_Amount_Live                  float64\n",
      "New_Loans_Last_3_Months                       int64\n",
      "Total_Secured_Sanctioned_Live               float64\n",
      "Total_Unsecured_Sanctioned_Live             float64\n",
      "Max_TwoWheeler_Loan_Amount                  float64\n",
      "Months_Since_Last_Personal_Loan             float64\n",
      "Months_Since_First_Consumer_Durable_Loan    float64\n",
      "DPD_30_Last_6_Months                          int64\n",
      "DPD_60_Last_6_Months                          int64\n",
      "DPD_90_Last_3_Months                          int64\n",
      "Tier                                         object\n",
      "Loan_Default                                  int64\n",
      "dtype: object\n",
      "\n",
      "Target Distribution:\n",
      " Loan_Default\n",
      "0    0.978131\n",
      "1    0.021869\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\psp17\\AppData\\Local\\Temp\\ipykernel_26176\\2333716700.py:2: DtypeWarning: Columns (9,12,13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('./TVS.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load CSV directly from same folder\n",
    "df = pd.read_csv('./TVS.csv')\n",
    "\n",
    "# Define new column names\n",
    "new_columns = [\n",
    "    \"Customer_ID\",\n",
    "    \"First_EMI_Bounced\",\n",
    "    \"Bounces_Last_12_Months\",\n",
    "    \"Max_MOB\",\n",
    "    \"Total_Bounces_On_Loan\",\n",
    "    \"EMI\",\n",
    "    \"Loan_Amount\",\n",
    "    \"Tenure\",\n",
    "    \"Dealer_Code\",\n",
    "    \"Product_Code\",\n",
    "    \"Advance_EMIs_Paid\",\n",
    "    \"Interest_Rate\",\n",
    "    \"Gender\",\n",
    "    \"Employment_Type\",\n",
    "    \"Resident_Type\",\n",
    "    \"DOB\",\n",
    "    \"Loan_Age_At_Application\",\n",
    "    \"Total_Loans\",\n",
    "    \"Secured_Loans\",\n",
    "    \"Unsecured_Loans\",\n",
    "    \"Max_Sanctioned_Amount_Live\",\n",
    "    \"New_Loans_Last_3_Months\",\n",
    "    \"Total_Secured_Sanctioned_Live\",\n",
    "    \"Total_Unsecured_Sanctioned_Live\",\n",
    "    \"Max_TwoWheeler_Loan_Amount\",\n",
    "    \"Months_Since_Last_Personal_Loan\",\n",
    "    \"Months_Since_First_Consumer_Durable_Loan\",\n",
    "    \"DPD_30_Last_6_Months\",\n",
    "    \"DPD_60_Last_6_Months\",\n",
    "    \"DPD_90_Last_3_Months\",\n",
    "    \"Tier\",\n",
    "    \"Loan_Default\"\n",
    "]\n",
    "\n",
    "# Rename columns\n",
    "df.columns = new_columns\n",
    "# Basic shape and types\n",
    "print(\"Shape of Dataset:\", df.shape)\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
    "print(\"\\nData Types:\\n\", df.dtypes)\n",
    "\n",
    "# Replace 'target' with your actual target column\n",
    "print(\"\\nTarget Distribution:\\n\", df['Loan_Default'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f10aca",
   "metadata": {},
   "source": [
    "### Dataset Overview\n",
    "\n",
    "- **Shape of Dataset:**  \n",
    "  The dataset contains 119,528 rows and 32 columns.\n",
    "\n",
    "- **Missing Values:**  \n",
    "  Several columns have a significant number of missing values (e.g., `Max_MOB`, `Total_Bounces_On_Loan`, `EMI`, etc. with 34,480 missing entries). Some columns like `Max_Sanctioned_Amount_Live` have even more missing values. We'll need to address these missing values during data cleaning.\n",
    "\n",
    "- **Data Types Warning:**  \n",
    "  There is a warning about mixed data types in columns 9, 12, 13, 14, and 15. This usually happens when a column contains both numbers and strings. We'll need to investigate and possibly convert these columns to the appropriate data types.\n",
    "\n",
    "- **Target Distribution:**  \n",
    "  The target variable `Loan_Default` is highly imbalanced:\n",
    "  - 0 (No Default): 97.81%\n",
    "  - 1 (Default): 2.19%\n",
    "\n",
    "  This class imbalance should be considered during model development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe5d79f",
   "metadata": {},
   "source": [
    "## Identify Numerical and Categorical Features\n",
    "\n",
    "Next, we'll separate the features into numerical and categorical columns. This distinction is crucial for data preprocessing, as different types of features often require different handling (e.g., scaling for numerical features, encoding for categorical features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86f77356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical Columns: ['Customer_ID', 'First_EMI_Bounced', 'Bounces_Last_12_Months', 'Max_MOB', 'Total_Bounces_On_Loan', 'EMI', 'Loan_Amount', 'Tenure', 'Dealer_Code', 'Advance_EMIs_Paid', 'Interest_Rate', 'Loan_Age_At_Application', 'Total_Loans', 'Secured_Loans', 'Unsecured_Loans', 'Max_Sanctioned_Amount_Live', 'New_Loans_Last_3_Months', 'Total_Secured_Sanctioned_Live', 'Total_Unsecured_Sanctioned_Live', 'Max_TwoWheeler_Loan_Amount', 'Months_Since_Last_Personal_Loan', 'Months_Since_First_Consumer_Durable_Loan', 'DPD_30_Last_6_Months', 'DPD_60_Last_6_Months', 'DPD_90_Last_3_Months']\n",
      "Categorical Columns: ['Product_Code', 'Gender', 'Employment_Type', 'Resident_Type', 'DOB', 'Tier']\n"
     ]
    }
   ],
   "source": [
    "# Separate columns\n",
    "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Remove target column\n",
    "target_col = 'Loan_Default'  # ðŸ” Replace if needed\n",
    "if target_col in num_cols: num_cols.remove(target_col)\n",
    "if target_col in cat_cols: cat_cols.remove(target_col)\n",
    "\n",
    "print(\"\\nNumerical Columns:\", num_cols)\n",
    "print(\"Categorical Columns:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765ee69",
   "metadata": {},
   "source": [
    "### Feature Types Identified\n",
    "\n",
    "- **Numerical Columns:**  \n",
    "  - Customer_ID, First_EMI_Bounced, Bounces_Last_12_Months, Max_MOB, Total_Bounces_On_Loan, EMI, Loan_Amount, Tenure, Dealer_Code, Advance_EMIs_Paid, Interest_Rate, Loan_Age_At_Application, Total_Loans, Secured_Loans, Unsecured_Loans, Max_Sanctioned_Amount_Live, New_Loans_Last_3_Months, Total_Secured_Sanctioned_Live, Total_Unsecured_Sanctioned_Live, Max_TwoWheeler_Loan_Amount, Months_Since_Last_Personal_Loan, Months_Since_First_Consumer_Durable_Loan, DPD_30_Last_6_Months, DPD_60_Last_6_Months, DPD_90_Last_3_Months\n",
    "\n",
    "- **Categorical Columns:**  \n",
    "  - Product_Code, Gender, Employment_Type, Resident_Type, DOB, Tier\n",
    "\n",
    "The target column (`Loan_Default`) has been excluded from both lists. This separation will help us apply the correct preprocessing steps to each feature type in the following stages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe259b8f",
   "metadata": {},
   "source": [
    "## Data Cleaning and Feature Engineering\n",
    "\n",
    "In this step, we perform several essential data cleaning and feature engineering operations to prepare the dataset for analysis and modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### Steps Performed\n",
    "\n",
    "1. **Drop `Customer_ID` Column**\n",
    "   - The `Customer_ID` column, which serves only as a unique identifier, is removed from the dataset as it does not provide predictive value.\n",
    "\n",
    "2. **Convert `Dealer_Code` to Categorical**\n",
    "   - The `Dealer_Code` column is explicitly converted to a string type to ensure it is treated as a categorical variable in subsequent analysis.\n",
    "\n",
    "3. **Convert `DOB` to Age**\n",
    "   - The `DOB` (Date of Birth) column is converted to datetime format.\n",
    "   - A new `Age` column is created by calculating the difference (in years) between the current date and the date of birth.\n",
    "   - The original `DOB` column is then dropped, as age is a more useful feature for modeling.\n",
    "\n",
    "   > **Note:**  \n",
    "   > A warning was generated during date parsing, indicating that the date format may be ambiguous. If your dates are in `day-month-year` format, consider specifying `dayfirst=True` in `pd.to_datetime()` for more accurate conversion.\n",
    "\n",
    "4. **Drop Features with More Than 70% Missing Values**\n",
    "   - Columns with more than 70% missing data are dropped from the dataset to avoid introducing bias or noise.\n",
    "   - The following columns were removed:\n",
    "     - `Total_Secured_Sanctioned_Live`\n",
    "     - `Total_Unsecured_Sanctioned_Live`\n",
    "     - `Months_Since_Last_Personal_Loan`\n",
    "     - `Months_Since_First_Consumer_Durable_Loan`\n",
    "\n",
    "5. **Fill Remaining Missing Values**\n",
    "   - **Numerical columns:** Missing values are filled with the median of each column.\n",
    "   - **Categorical columns:** Missing values are filled with the string `'Missing'`.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a44137d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\psp17\\AppData\\Local\\Temp\\ipykernel_26176\\3349965331.py:17: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['DOB'] = pd.to_datetime(df['DOB'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns with >70% missing: ['Total_Secured_Sanctioned_Live', 'Total_Unsecured_Sanctioned_Live', 'Months_Since_Last_Personal_Loan', 'Months_Since_First_Consumer_Durable_Loan']\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Drop Customer_ID if it exists ---\n",
    "if 'Customer_ID' in df.columns:\n",
    "    df.drop('Customer_ID', axis=1, inplace=True)\n",
    "    if 'Customer_ID' in num_cols:\n",
    "        num_cols.remove('Customer_ID')\n",
    "\n",
    "# --- Step 2: Convert Dealer_Code to categorical if it exists ---\n",
    "if 'Dealer_Code' in df.columns:\n",
    "    df['Dealer_Code'] = df['Dealer_Code'].astype(str)\n",
    "    if 'Dealer_Code' not in cat_cols:\n",
    "        cat_cols.append('Dealer_Code')\n",
    "    if 'Dealer_Code' in num_cols:\n",
    "        num_cols.remove('Dealer_Code')\n",
    "\n",
    "# --- Step 3: Convert DOB to Age if it exists ---\n",
    "if 'DOB' in df.columns:\n",
    "    df['DOB'] = pd.to_datetime(df['DOB'], errors='coerce')\n",
    "    df['Age'] = (pd.to_datetime('today') - df['DOB']).dt.days // 365\n",
    "    df.drop('DOB', axis=1, inplace=True)\n",
    "    if 'DOB' in cat_cols:\n",
    "        cat_cols.remove('DOB')\n",
    "    if 'Age' not in num_cols:\n",
    "        num_cols.append('Age')\n",
    "\n",
    "# --- Step 4: Drop features with >70% missing ---\n",
    "threshold = 0.7\n",
    "missing_ratio = df.isnull().sum() / len(df)\n",
    "to_drop = missing_ratio[missing_ratio > threshold].index.tolist()\n",
    "df.drop(columns=to_drop, inplace=True)\n",
    "print(\"Dropped columns with >70% missing:\", to_drop)\n",
    "\n",
    "# Update column lists\n",
    "num_cols = [col for col in num_cols if col in df.columns]\n",
    "cat_cols = [col for col in cat_cols if col in df.columns]\n",
    "\n",
    "# --- Step 5: Fill missing values safely ---\n",
    "# Fill numeric\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "# Fill categorical only on columns that still exist\n",
    "for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna('Missing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e59189",
   "metadata": {},
   "source": [
    "### Output Summary\n",
    "\n",
    "- **Dropped columns with >70% missing:**  \n",
    "  - `Total_Secured_Sanctioned_Live`\n",
    "  - `Total_Unsecured_Sanctioned_Live`\n",
    "  - `Months_Since_Last_Personal_Loan`\n",
    "  - `Months_Since_First_Consumer_Durable_Loan`\n",
    "\n",
    "- **Date Parsing Warning:**  \n",
    "  - A warning was issued regarding the date format in the `DOB` column. Double-check your date format and adjust the parsing parameters if necessary.\n",
    "\n",
    "---\n",
    "\n",
    "These cleaning and transformation steps ensure that the dataset is ready for further exploration, feature selection, and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4702c00",
   "metadata": {},
   "source": [
    "## Data Cleaning Results and Feature Overview\n",
    "\n",
    "After completing the data cleaning and feature engineering steps, let's review the current state of the dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62b4a033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values: 0\n",
      "Final shape: (119528, 27)\n",
      "Numerical columns: ['First_EMI_Bounced', 'Bounces_Last_12_Months', 'Max_MOB', 'Total_Bounces_On_Loan', 'EMI', 'Loan_Amount', 'Tenure', 'Advance_EMIs_Paid', 'Interest_Rate', 'Loan_Age_At_Application', 'Total_Loans', 'Secured_Loans', 'Unsecured_Loans', 'Max_Sanctioned_Amount_Live', 'New_Loans_Last_3_Months', 'Max_TwoWheeler_Loan_Amount', 'DPD_30_Last_6_Months', 'DPD_60_Last_6_Months', 'DPD_90_Last_3_Months', 'Age']\n",
      "Categorical columns: ['Product_Code', 'Gender', 'Employment_Type', 'Resident_Type', 'Tier', 'Dealer_Code']\n"
     ]
    }
   ],
   "source": [
    "print(\"Remaining missing values:\", df.isnull().sum().sum())\n",
    "print(\"Final shape:\", df.shape)\n",
    "print(\"Numerical columns:\", num_cols)\n",
    "print(\"Categorical columns:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd9be5",
   "metadata": {},
   "source": [
    "### Output Summary\n",
    "\n",
    "- **Remaining Missing Values:**  \n",
    "  All missing values have been successfully handled. There are now **0 missing values** in the dataset.\n",
    "\n",
    "- **Final Shape of the Dataset:**  \n",
    "  - **Rows:** 119,528  \n",
    "  - **Columns:** 27\n",
    "\n",
    "---\n",
    "\n",
    "### Updated Feature Lists\n",
    "\n",
    "- **Numerical Columns:**  \n",
    "  - First_EMI_Bounced\n",
    "  - Bounces_Last_12_Months\n",
    "  - Max_MOB\n",
    "  - Total_Bounces_On_Loan\n",
    "  - EMI\n",
    "  - Loan_Amount\n",
    "  - Tenure\n",
    "  - Advance_EMIs_Paid\n",
    "  - Interest_Rate\n",
    "  - Loan_Age_At_Application\n",
    "  - Total_Loans\n",
    "  - Secured_Loans\n",
    "  - Unsecured_Loans\n",
    "  - Max_Sanctioned_Amount_Live\n",
    "  - New_Loans_Last_3_Months\n",
    "  - Max_TwoWheeler_Loan_Amount\n",
    "  - DPD_30_Last_6_Months\n",
    "  - DPD_60_Last_6_Months\n",
    "  - DPD_90_Last_3_Months\n",
    "  - Age\n",
    "\n",
    "- **Categorical Columns:**  \n",
    "  - Product_Code\n",
    "  - Gender\n",
    "  - Employment_Type\n",
    "  - Resident_Type\n",
    "  - Tier\n",
    "  - Dealer_Code\n",
    "\n",
    "---\n",
    "\n",
    "The dataset is now fully cleaned, with all missing values addressed and features properly categorized. This prepares the data for further analysis, feature selection, and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b07ce2d",
   "metadata": {},
   "source": [
    "## Finalize Categorical Feature List\n",
    "\n",
    "To ensure there are no duplicate entries in our list of categorical columns, we remove any duplicates and display the final set of categorical features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d738be02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final categorical columns: ['Resident_Type', 'Dealer_Code', 'Product_Code', 'Tier', 'Employment_Type', 'Gender']\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates in cat_cols\n",
    "cat_cols = list(set(cat_cols))\n",
    "print(\"âœ… Final categorical columns:\", cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c7fdda",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "- **Final categorical columns:**  \n",
    "  - Resident_Type\n",
    "  - Dealer_Code\n",
    "  - Product_Code\n",
    "  - Tier\n",
    "  - Employment_Type\n",
    "  - Gender\n",
    "\n",
    "---\n",
    "\n",
    "With this, our list of categorical columns is clean and ready for encoding or further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0593e943",
   "metadata": {},
   "source": [
    "## Encode Categorical Features for Exploratory Data Analysis (EDA)\n",
    "\n",
    "To facilitate numerical analysis and visualization, we encode the categorical features using Label Encoding. This step converts each category into a unique integer, making it easier to work with these features in statistical and machine learning methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b402a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded categorical columns:    Resident_Type  Dealer_Code  Product_Code  Tier  Employment_Type  Gender\n",
      "0              2          280             4     0                0       0\n",
      "1              3         1439             1     0                4       1\n",
      "2              3          249             4     0                4       1\n",
      "3              3          271             4     0                3       0\n",
      "4              3         2506             0     0                3       1\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical features for EDA\n",
    "df_encoded = df.copy()\n",
    "le_dict = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df[col])\n",
    "    le_dict[col] = le\n",
    "\n",
    "print(\"Encoded categorical columns:\", df_encoded[cat_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d08310",
   "metadata": {},
   "source": [
    "### What was done:\n",
    "- Created a copy of the original dataframe to preserve raw data.\n",
    "- Applied `LabelEncoder` to each categorical column.\n",
    "- Stored each encoder in a dictionary (`le_dict`) for potential inverse transformations later.\n",
    "\n",
    "---\n",
    "The encoded dataframe (`df_encoded`) is now ready for numerical analysis and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053fc095",
   "metadata": {},
   "source": [
    "## Prepare Data for Modeling and Define Gini Score\n",
    "\n",
    "In this step, we set up the data for model training and evaluation, and define a custom Gini score functionâ€”a common metric for assessing model performance in imbalanced classification problems.\n",
    "\n",
    "---\n",
    "\n",
    "### Steps Performed\n",
    "\n",
    "1. **Import Required Libraries**\n",
    "   - Imported essential modules for modeling, evaluation, and data splitting:\n",
    "     - `LogisticRegression` for building a baseline model.\n",
    "     - `roc_auc_score` for calculating the Area Under the ROC Curve (AUC).\n",
    "     - `train_test_split` for splitting the data into training and testing sets.\n",
    "     - `deepcopy` and `numpy` for data manipulation and copying.\n",
    "\n",
    "2. **Prepare Features and Target**\n",
    "   - `X`: All features except the target (`Loan_Default`).\n",
    "   - `y`: The target variable (`Loan_Default`).\n",
    "\n",
    "3. **Train-Test Split**\n",
    "   - Split the data into training and testing sets using an 80/20 ratio.\n",
    "   - Used stratified sampling to maintain the class distribution in both sets.\n",
    "   - Set a random seed (`random_state=42`) for reproducibility.\n",
    "\n",
    "4. **Define Gini Score Function**\n",
    "   - Created a function `gini_score` that computes the Gini coefficient based on the ROC AUC score:\n",
    "     - \\( \\text{Gini} = 2 \\times \\text{AUC} - 1 \\)\n",
    "   - The Gini coefficient is widely used in credit risk modeling to measure discriminatory power.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0bb08966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "# Setup data\n",
    "X = df_encoded.drop('Loan_Default', axis=1)\n",
    "y = df_encoded['Loan_Default']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gini Score Function\n",
    "def gini_score(y_true, y_pred_proba):\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    return 2 * auc - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b97860b",
   "metadata": {},
   "source": [
    "The data is now ready for model training and evaluation, with a custom Gini score function defined for performance measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d7819c",
   "metadata": {},
   "source": [
    "## Baseline Model: Logistic Regression with Feature Scaling\n",
    "\n",
    "In this step, we build a baseline predictive model using logistic regression, incorporating feature scaling to ensure all features contribute equally to the model.\n",
    "\n",
    "---\n",
    "\n",
    "### Steps Performed\n",
    "\n",
    "1. **Import Required Modules**\n",
    "   - Imported `StandardScaler` for feature scaling.\n",
    "   - Imported `make_pipeline` to streamline preprocessing and modeling.\n",
    "\n",
    "2. **Pipeline Construction**\n",
    "   - Created a pipeline that first scales all features using `StandardScaler`, then fits a logistic regression model (`max_iter=1000` for convergence).\n",
    "\n",
    "3. **Model Training and Prediction**\n",
    "   - Trained the pipeline on the training data.\n",
    "   - Predicted the probability of default for the test set.\n",
    "\n",
    "4. **Performance Evaluation**\n",
    "   - Calculated the Gini score using the previously defined `gini_score` function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "424b74aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Baseline Gini Score with all features (scaled): 0.5366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# Scale + Train logistic regression\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "baseline_gini = gini_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"ðŸš€ Baseline Gini Score with all features (scaled): {baseline_gini:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2180e5",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "- **Baseline Gini Score with all features (scaled):**  \n",
    "  **0.5366**\n",
    "\n",
    "---\n",
    "\n",
    "This baseline Gini score provides a reference point for evaluating the impact of future feature selection, engineering, or model improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb321db0",
   "metadata": {},
   "source": [
    "## Feature Importance via Gini Drop Analysis\n",
    "\n",
    "In this step, we assess the importance of each feature by measuring the change in Gini score when each feature is individually removed from the model. This helps identify which features contribute most to the model's predictive power.\n",
    "\n",
    "---\n",
    "\n",
    "### Methodology\n",
    "\n",
    "- For each feature, we:\n",
    "  - Remove the feature from the dataset.\n",
    "  - Retrain the logistic regression model (with scaling) on the remaining features.\n",
    "  - Calculate the Gini score on the test set.\n",
    "  - Compute the difference (`Gini_Drop`) between the baseline Gini and the new Gini score.\n",
    "- Features are then sorted by the smallest Gini drop, indicating the least impact on model performance when dropped.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df67cbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Features sorted by least Gini impact when dropped:\n",
      "\n",
      "                       Feature  Gini_Without  Gini_Drop\n",
      "12             Employment_Type      0.541615  -0.005057\n",
      "4                          EMI      0.540420  -0.003861\n",
      "13               Resident_Type      0.539493  -0.002934\n",
      "7                  Dealer_Code      0.538816  -0.002257\n",
      "24                        Tier      0.538226  -0.001667\n",
      "20  Max_TwoWheeler_Loan_Amount      0.537722  -0.001164\n",
      "5                  Loan_Amount      0.537694  -0.001135\n",
      "1       Bounces_Last_12_Months      0.537504  -0.000945\n",
      "0            First_EMI_Bounced      0.537209  -0.000650\n",
      "16               Secured_Loans      0.536933  -0.000375\n",
      "2                      Max_MOB      0.536851  -0.000292\n",
      "19     New_Loans_Last_3_Months      0.536559   0.000000\n",
      "8                 Product_Code      0.535953   0.000606\n",
      "10               Interest_Rate      0.535855   0.000704\n",
      "22        DPD_60_Last_6_Months      0.535305   0.001253\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "feature_gini_drop = []\n",
    "\n",
    "# Go one-by-one and drop each feature\n",
    "for feature in X.columns:\n",
    "    X_temp = X.drop(columns=[feature])\n",
    "    \n",
    "    # Split again (same y used)\n",
    "    X_train_temp, X_test_temp, _, _ = train_test_split(X_temp, y, stratify=y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Use pipeline: StandardScaler + LogisticRegression\n",
    "    pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "    pipe.fit(X_train_temp, y_train)\n",
    "    y_pred_temp = pipe.predict_proba(X_test_temp)[:, 1]\n",
    "    \n",
    "    gini = gini_score(y_test, y_pred_temp)\n",
    "    delta = baseline_gini - gini\n",
    "    feature_gini_drop.append((feature, gini, delta))\n",
    "\n",
    "# Create DataFrame of Gini drops\n",
    "drop_df = pd.DataFrame(feature_gini_drop, columns=['Feature', 'Gini_Without', 'Gini_Drop'])\n",
    "drop_df = drop_df.sort_values(by='Gini_Drop')\n",
    "\n",
    "# Show result\n",
    "print(\"ðŸ“‰ Features sorted by least Gini impact when dropped:\\n\")\n",
    "print(drop_df.head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb80390e",
   "metadata": {},
   "source": [
    "### Top 15 Features with Least Gini Impact When Dropped\n",
    "\n",
    "| Feature                      | Gini Without | Gini Drop  |\n",
    "|------------------------------|--------------|------------|\n",
    "| Employment_Type              | 0.5416       | -0.0051    |\n",
    "| EMI                          | 0.5404       | -0.0039    |\n",
    "| Resident_Type                | 0.5395       | -0.0029    |\n",
    "| Dealer_Code                  | 0.5388       | -0.0023    |\n",
    "| Tier                         | 0.5382       | -0.0017    |\n",
    "| Max_TwoWheeler_Loan_Amount   | 0.5377       | -0.0012    |\n",
    "| Loan_Amount                  | 0.5377       | -0.0011    |\n",
    "| Bounces_Last_12_Months       | 0.5375       | -0.0009    |\n",
    "| First_EMI_Bounced            | 0.5372       | -0.0007    |\n",
    "| Secured_Loans                | 0.5369       | -0.0004    |\n",
    "| Max_MOB                      | 0.5369       | -0.0003    |\n",
    "| New_Loans_Last_3_Months      | 0.5366       | 0.0000     |\n",
    "| Product_Code                 | 0.5360       | 0.0006     |\n",
    "| Interest_Rate                | 0.5359       | 0.0007     |\n",
    "| DPD_60_Last_6_Months         | 0.5353       | 0.0013     |\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Negative Gini Drop:** Indicates that removing the feature actually increased the Gini score slightly, suggesting the feature may not be helpful or could introduce noise.\n",
    "- **Near-Zero Gini Drop:** Features with a Gini drop close to zero have minimal impact on model performance and could be candidates for removal or further investigation.\n",
    "- **Larger Gini Drop:** Features with a larger (positive) Gini drop are more important for the model's predictive power.\n",
    "\n",
    "---\n",
    "\n",
    "This analysis provides a data-driven basis for feature selection, helping to streamline the model and potentially improve generalization by removing less useful features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
